# 渲染知识笔记    

## SSAO（屏幕空间环境光遮蔽）   
### 基本定义 
Ambient Occlusion: 通过场景中的某个点来产生一个标量值，用来表示从该点向各个方向发出的射线被遮挡的概率。该标量值用来产生全局的遮挡效果，为用户提供关于场景中物体位置关系和表面起伏情况的视觉信息。AO的计算通过半球面上可见性积分得到，具体计算方式：[AO计算原理](https://zhuanlan.zhihu.com/p/46633896)  
SSAO是一种基于屏幕空间的AO算法   
### SSAO算法  
AO的半球积分计算复杂度高，一般应用于离线渲染，在实时渲染中需要计算效率更高的算法，而SSAO是实时渲染中常用的算法。使用当前视点的深度缓存当成场景中物体遮挡关系的粗略近似代替光线来求AO。
### SSAO特点    
- 与场景复杂度不相关，与场景中的顶点数和三角形没有关系，仅和投影后的像素有关   
- 相比于传统AO，无需预处理，无需加载时间，和分配内存，适用于动态场景  
- 对每个像素的处理流程相同，可以由GPU进行并行处理
- 可以集成到现代渲染管线中   
### SSAO基本步骤   
对屏幕空间里的像素计算在三维空间的位置点 p :     

- 以 p 为球心，R为半径的球体内随机产生三维采样点  
- 估算每个采样点的AO：取得采样点在深度缓存中的投影点的AO遮蔽近似代替采样点的遮蔽  
- 投影点遮蔽的计算方法，不同的算法计算方式不同，最简单直接的是以投影点的深度值与p的深度值差异作为近似遮蔽，这样会有自身遮蔽走样问题，一种改进方法是引入法向缓存作为判断，使所有的采样点都在P点的上方，并利用采样点与投影点的深度值差异作为遮蔽近似    
- 平均所有采样点的遮蔽值    

SSAO所有计算都发生在屏幕空间，其计算效率较高，但在其精度上比不上基于顶点的AO计算结果，因此SSAO是牺牲了一定精度换取实时渲染的速度       

---
## IBL (基于图像光照)  
### 基本定义  
IBL基于图像的光照是实现丰富环境光照效果的一种方式，一般是通过一张环境贴图(EnvironmentMap)来保存周围环境信息，通过采样处理得到另一张环境贴图实现丰富的环境光照效果  
### 实现推导过程  
[IBL计算推导](https://zhuanlan.zhihu.com/p/95865910)  
### 计算步骤  
- 获得环境贴图  
- 预计算辐射光照贴图   
输入为环境纹理，输出也是一张环境纹理，计算过程中使用球形坐标系简化计算方程，使用黎曼和求解积分  
- IBL光照计算    
片元法线采样来计算采样   
参考： [基于物理的渲染PBR之基于图像的光照IBL](https://blog.csdn.net/qjh5606/article/details/89948573)     


## GPU渲染管线及硬件架构笔记     
文章来源：[GPU 渲染管线和硬件架构浅谈](https://mp.weixin.qq.com/s/-ueKhxbsJOnUtV1SC5eyBQ)  

### GPU渲染管线     
---
**桌面端GPU渲染管线及架构**   
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/IMR%20pipeline.jpeg "IMR渲染架构")    
桌面端常见GPU架构，每个drawcall中，GPU按照顺序处理图元  
- 应用阶段：粗粒度剔除（遮挡剔除），渲染状态设置，顶点着色数据准备 （CPU内完成）   
- 几何阶段（顶点处理阶段）：顶点着色器，曲面细分，几何着色器，顶点裁剪，屏幕映射，该过程会做背面剔除裁剪    
- 光栅化阶段：三角形设置，三角形遍历，片元着色器   
- 逐片元操作：裁剪测试，深度测试，模板测试，混合操作  

- **优势**：   
渲染管线连续，有利于提高GPU最大吞吐量，最大化利用GPU性能，从vertex到raster处理都可以在GPU内部on-chip buffer上进行，所以只需少量带宽，存取处理过程中图元数据。     
- **劣势**：  
IMR是全屏绘制，当前绘制图元可能是屏幕的任意位置，因此需要全屏buffer，该buffer内存很大，只能放在系统内存中，这样在与系统内存buffer的数据进行交互时，需要消耗大量带宽，对于移动端来说过于昂贵。   

---  
**移动端渲染管线及架构**            
对于移动端，控制功耗很重要，功耗高带来发热降频，会是画面出现严重卡顿和掉帧。带宽的大量消耗会造成明显耗电和发热。因此移动端对于功耗控制尤为谨慎，普遍使用 TBR/TBDR 

- TBR:Tile-based Rendering      
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/TBR%20pipeline.jpeg)     
TBR不绘制整个屏幕，而是将屏幕分成不同的Tile，GPU每次只绘制一个Tile，绘制完后再将绘制结果写入系统内存的frameBuffer，处理过程：   
1. 处理所有的顶点，生成tile list中间数据，该数据保存顶点归属于屏幕的哪一个Tile   
2. 针对每一个Tile执行光栅化，片元着色器过程，每个Tile处理完毕之后，写入系统内存中     
- **优势**   
TBR减少了对系统内存的访问，减少了带宽开销，Tile足够小，可以在GPU on-chip-memory 上，访问速度快，后续的片元着色器阶段同样可以在 on-chip-memory上处理，depth buffer 和 stencil buffer 都只在Tile中处理才会用到，不用写入系统内存，进一步节省带宽      
- **劣势**   
TBR需要先对所有顶点进行处理，生成tile list ，这一步会产生明显的延迟，顶点越多消耗就越大，对于曲面细分在TBR下会显得昂贵，顶点量过大会带来性能严重下降        

- TBDR：Tile-based Deferred Rendering  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/TBDR%20pipeline.jpeg)    
TBDR 和 TBR 架构很类似，区别在于，TBDR多了一个隐面剔除，即 HSR&Depth Test这个步骤，通过HSR，最终只有对屏幕产生贡献的像素会执行片元着色器，被遮挡的片元会被丢弃，避免执行无效的片元着色器    
---   
### GPU 硬件架构     
---
**GPU 与 CPU 差异**     
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU_CPU差异.jpeg)      
- CPU 核心少（计算单元少，内部结构复杂），每个核心有控制单元和独立缓存，设计上是大缓存，要求低延时  
- GPU 计算单元多，多个计算单元共享控制单元和缓存，内存设计上是追求高带宽，可以接受高延时   
- CPU经常用来执行串行和计算复杂度高的任务，GPU则用来处理高并行，相互独立，复杂度低的任务，通过多核心来抵消任务处理过程中的任务排队时间     
---   
**CPU的缓存体系和指令执行过程**   
内存硬件类型     
- SRAM (Static Random Access Memory,静态随机存取内存) 静止存取数据作用，无需刷新电路即可保存数据，断电后数据消失，速度较DRAM快，一般用作片内缓存(on-chip-cache)，如L1,L2 Cache      
- DRAM（Dynamic Random Access Memory,动态随机存取内存）需要不断刷新电路，否则数据会消失。常用于做内存，容量比SRAM大，一般用作系统内存，桌面端目前都是DDR SDRAM(Double Data Rate Synchronous Dynamic Random-Access Memory)   
- GDDR (Graphic DDR) 用作显存，时钟频率更高，耗电量更少    
- LPDDR SDRAM (Low Power Double Date Rate),是移动设备常用低功耗SDRAM，以低功耗小体积著称，Frame Buffer存放于此     
- UFS 通用闪存存储，移动磁盘设备   

CPU缓存体系   
CPU有L1/L2/L3 三级缓存，L1与L2缓存在核心内部，L3为所有核心共享，缓存是SRAM，比系统内存DRAM速度快很多  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/CPU缓存.jpeg)    
- L1/L2 缓存是片上缓存，读取速度快，容量小。L1通常在32Kb - 256Kb,而L3缓存可以达到8Mb - 32Mb级别   
- L1级别缓存分为指令缓存(I-Cache)和数据缓存(D-Cache)，CPU针对指令和数据有不同缓存策略 
- L1缓存不能设计太大，增大L1缓存，会增加访问的时钟周期，对于低延时来讲，降低了L1 Cache性能  
- CPU的L1/L2缓存需要处理缓存一致性问题，不同核心之间L1缓存之间数据一致，一个核心中的L1缓存数据发生变化，其他核心中相应数据标记为无效，而GPU的缓存无需处理该问题（GPU缓存主要用来提高线程服务，而不是存储数据）   
- CPU的数据查找按照 L1->L2->L3->DRAM的顺序进行，当数据不在缓存中时，需要从系统内存中加载，会造成较大延时  
- 缓存对于CPU性能非常重要，很多时候缓存会占据芯片一半以上面积和晶体管。苹果A14/A5/M1 芯片性能的强劲与超大缓存密不可分     
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/CPU结构.jpeg)       
---
CPU指令执行过程  
- 指令流水线五个阶段   
    - 取指令(Instruction Fetch)从指令缓存(I-Cache)中取指令放入指令寄存器中    
    - 解码指令(Instruction Decode)，这里还通过寄存器文件(Register File)取到指令的源操作数    
    - 执行指令(Execute)  
    - 访问数据(Memory Access) 如果需要存储器取数据，通过数据缓存(D-Cache)取数据  
    - 结果写入(Register Write Back)将指令执行结果写入寄存器    

- 访问主存，CPU和GPU通常会出现较大延迟，面对延迟，CPU和GPU处理策略不同。CPU通过大容量高速缓存，分支预测，乱序执行遮掩延迟，CPU缓存容量大于GPU，GPU没有L3级缓存，且CPU缓存是以低延迟为目标设计。GPU则是通过大量的计算单元切换线程Wrap来规避延迟。   
- 分支，CPU通过分支预测，提高指令流水线的执行性能，GPU无分支预测单元，不擅长执行分支（在Shader中执行分支操作耗性能）。   
- 超标量设计，CPU可以同时发射多条指令，并使指令并行计算，充分利用计算单元。 
- 乱序执行，若按照原有指令顺序执行，可能指令之间有依赖无法并行执行，或者频繁出现高延迟指令。CPU会保证执行结果正确基础上，调整指令执行顺序，使指令更加高效执行，减少执行等待，提高管线性能。    
- 线程切换，CPU线程切换会有上下文切换的性能开销（线程切换需要保存寄存器和程序计数器，切换回来时需要恢复寄存器和程序计数器）CPU尽量避免频繁线程切换。GPU寄存器数量多，线程切换时不需要保存上下文，所以可以通过零成本切换线程来遮掩延迟。   
---
桌面端 GPU 硬件架构     
![NVIDIA Fermi架构](https://github.com/zhangbo-0213/PictureRepository/blob/main/NvidiaFermi.jpeg)   
不同的GPU，架构差异较大，一般包含以下核心组件：    
- SM，SMX，SMM(Streaming Multiprocessor)。GPU核心，执行 Shader 指令的地方，一个GPU由多个SM组成。Mali种类似单元叫 Shader Core，PowerVR 中为 Unified Shading Cluster(USC)。  
- Core 真正执行指令的地方，NVIDIA 中为CUDA Core  
- Raster Engine 光栅化引擎    
- ROP(Raster Operation) depth testing blending 等操作在这里完成   
- Register File ,L1 Cache L2 Cache 寄存器和各级缓存    
---
Shader Core 主要构成单元    
- 32个 运算核心 (CUDA Core,也叫 流处理器 Stream processor)    
- 16个 LD/ST (Load/Store) 模块来加载和存储数据 
- 4个 SFU(Special function units) 执行特殊数学计算(sin,cos,log)等    
- 128K 寄存器 (Register File) 3万个 32-Bit 寄存器，大寄存器设计      
- 64K L1缓存 (on-chip Memory)  
- 纹理读取单元 (Texture Unit)  
- 指令缓存 (Instruction Cache)  
- Wrap Schedules:这个模块负责 Wrap 调度,一个wrap由32个线程组成，wrap调度器的指令Dispatch Unit 送到 Core 执行 
---
GPU 内存结构     
- UMA(Unified Memory Architeture)        
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU内存结构.jpeg)    
  - 左图为桌面端独显的分离式架构，CPU和GPU使用独立的物理内存，右侧为移动端的统一内存架构，CPU与GPU共用一个物理内存     
  - UMA 并不是指CPU，GPU内存合并在一起，实际上他们使用的内存区域不一样，物理内存中有一块区域由GPU自己管理。CPU和GPU的数据通信依然有拷贝过程。   
  - 移动芯片都是Soc (System on Chip),CPU和GPU等元件在一个芯片上，芯片面积不能做的很大，不能像桌面端为显卡配置GDDR显存，并且通过独立的北桥 PCI-e 通信。移动端CPU和GPU使用同一个物理内存，更加灵活，操作系统可以决定分配给GPU的显存大小，但是会造成CPU和GPU抢占带宽，会进一步限制GPU能使用的带宽     
  - GPU使用独立的显存空间可以对Buffer 或者 Texture 做进一步优化   

GPU 缓存分类      

- GPU 缓存结构  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU缓存结构.jpeg)     
  - L1缓存是片上缓存(on-chip)，每个 shader 核心都有自己独立的 L1 缓存，访问速度快。移动GPU会有Tile Memory，也就是片上内存(on-chip memory)   
  - L2缓存是所有shader核心共享，属于片外缓存，距离shader核心比L1缓存远，访问速度较L1缓存慢   
  - DRAM 是主存(系统内存，System Memory)，访问速度最慢，Frame Buffer放在主存上。   

- 内存访问速度    
内存访问速度从寄存器到主存依次变慢    
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/内存访问速度.png)     
  - 寄存器访问速度是最快的，GPU的寄存器数量很多    
  - Shared Memory和L1 Cache是同一个硬件单元，Shad red Memory 是可以由开发者控制的片上内存，L1缓存是GPU控制的，开发者无法访问，部分移动端芯片没有Shared Memory   
  - Local Memory和Texture/Const Memory 都是主存上的内存区域，访问速度比较慢      

NVIDIA 的内存分类      
NVIDIA 的内存分类主要是针对 CUDA 开发的，与游戏开发或者移动GPU的分类叫法存在部分差异     
- 全局内存(Global memory),即主存  
- 本地内存(LOcal memory),是Global中的一部分，每个线程私有，主要用于处理寄存器溢出，或者超大uniform数组，访问速度很慢   
- 共享内存(Shared memory),Shared memory 是片上内存，访问速度较快，是一个shader核心内所有线程共享   
- 寄存器内存(Register memory)，访问速度最快   
- 常量内存(Const memory),Const memory和Local memory类似，是Global memory 中的一块区域，访问速度较慢    
- 纹理内存(Texture memory),与 Const memory一样，也在主存上         

Cache line  
 - GPU 和缓存之间的内存交换不是以字节为单位，而是以 Cache line为单位，Cache line是固定的大小，比如CPU的Cache line 是64字节，GPU是128字节     
 - Cache Line的数据内容为：标记位+地址偏移+实际数据，标记位是为了确定缓存是否命中，是否写入主存    

Memory Bank 和 Bank Conflict   
 - 为提高内存的访问性能，获得更高的带宽，Shared Memory/L Cache 被设计成更小块的bank区域（bank可以理解为Memory 的对外窗口，多个窗口可访问比只有一个窗口要高效），bank 数量一般与wrap大小或者CUDA core的数量对应，如32个core就把SMEM划分为32个bank,每个bank包含多个cache line      
 - 如果一个wrapz中的不同线程访问的是不同的bank,那么可以并行执行，最大化利用带宽，提高性能    
 - 如果访问的是同一个bank中的同一个cache line，通过广播机制同步到其他线程，这样一次访问，其他线程都能获取到数据，不会存在性能问题   
 - 如果访问的是同一个bank上的不同cache line，会形成阻塞，必须阻塞等待，串行访问，这种情况会阻碍GPU的并行性，性能会下降，被称为 bank conflict   
 - 如果不同线程对同一个cache line 产生写入操作，也必须阻塞等待，串行写入        
 ---










