# 渲染知识笔记    
## 部分基础概念  
### Render Target  
- 计算机图形领域，渲染目标是现代图形处理单元(GPU)的一个特性，它允许将3D场景渲染的结果存储到中间存储缓冲区或者是一张纹理上(RTT)，而不是帧前缓冲区或者帧后缓冲区，然后通过像素着色器(片元着色器)对该结果操作，以便在显示最终图像之前将其他效果应用于最终图像     ——————维基百科     
- 也就是说，渲染目标是一个缓冲（区别于 screen frame buffer 和 back frame buffer之外的缓冲区），是用来记录暂存渲染结果，而不是直接绘制到屏幕，而是使用在其他地方。离屏渲染就可以通过render target来实现。  
- 需要注意，纹理不是渲染缓冲区，渲染缓冲区也不是纹理。帧缓冲区可以使用他们中的任何一个作为渲染目标。    
### SwapChain    
- 场景在渲染过程中，为了避免在绘制过程中出现闪烁，通常处理是将一帧的绘制过程在屏幕外进行，屏幕上显示已经绘制完成的完整帧图像，通过这种方式，观察者看不到帧绘制过程，只能看到之前已经绘制完成的帧。为此，硬件设置了两个缓存，一个是前置缓存(front/screen frame buffer)，一个是后置缓存(back frame buffer)。   
- 前置缓存执行当前显示在屏幕上的图形数据，与此同时，下一帧在后置缓存中计算。  
- 绘制完成后，两个缓存关系翻转，前置变后置，后置变前置，循环操作，该过程称为呈现(presenting/swap)。
- 呈现过程是高效的，只需要前置缓存和后置缓存指针来回交换。  
- 除了双缓存设计，还有三缓存设计方式。 

---
## SSAO（屏幕空间环境光遮蔽）   
### 基本定义 
Ambient Occlusion: 通过场景中的某个点来产生一个标量值，用来表示从该点向各个方向发出的射线被遮挡的概率。该标量值用来产生全局的遮挡效果，为用户提供关于场景中物体位置关系和表面起伏情况的视觉信息。AO的计算通过半球面上可见性积分得到，具体计算方式：[AO计算原理](https://zhuanlan.zhihu.com/p/46633896)  
SSAO是一种基于屏幕空间的AO算法   
### SSAO算法  
AO的半球积分计算复杂度高，一般应用于离线渲染，在实时渲染中需要计算效率更高的算法，而SSAO是实时渲染中常用的算法。使用当前视点的深度缓存当成场景中物体遮挡关系的粗略近似代替光线来求AO。
### SSAO特点    
- 与场景复杂度不相关，与场景中的顶点数和三角形没有关系，仅和投影后的像素有关   
- 相比于传统AO，无需预处理，无需加载时间，和分配内存，适用于动态场景  
- 对每个像素的处理流程相同，可以由GPU进行并行处理
- 可以集成到现代渲染管线中   
### SSAO基本步骤   
对屏幕空间里的像素计算在三维空间的位置点 p :     

- 以 p 为球心，R为半径的球体内随机产生三维采样点  
- 估算每个采样点的AO：取得采样点在深度缓存中的投影点的AO遮蔽近似代替采样点的遮蔽  
- 投影点遮蔽的计算方法，不同的算法计算方式不同，最简单直接的是以投影点的深度值与p的深度值差异作为近似遮蔽，这样会有自身遮蔽走样问题，一种改进方法是引入法向缓存作为判断，使所有的采样点都在P点的上方，并利用采样点与投影点的深度值差异作为遮蔽近似    
- 平均所有采样点的遮蔽值    

SSAO所有计算都发生在屏幕空间，其计算效率较高，但在其精度上比不上基于顶点的AO计算结果，因此SSAO是牺牲了一定精度换取实时渲染的速度       

---
## IBL (基于图像光照)  
### 基本定义  
IBL基于图像的光照是实现丰富环境光照效果的一种方式，一般是通过一张环境贴图(EnvironmentMap)来保存周围环境信息，通过采样处理得到另一张环境贴图实现丰富的环境光照效果  
### 实现推导过程  
[IBL计算推导](https://zhuanlan.zhihu.com/p/95865910)  
### 计算步骤  
- 获得环境贴图  
- 预计算辐射光照贴图   
输入为环境纹理，输出也是一张环境纹理，计算过程中使用球形坐标系简化计算方程，使用黎曼和求解积分  
- IBL光照计算    
片元法线采样来计算采样   
参考： [基于物理的渲染PBR之基于图像的光照IBL](https://blog.csdn.net/qjh5606/article/details/89948573)     
---
## HDR、颜色分级、颜色映射与颜色空间
[后处理下的HDR、颜色分级、颜色映射与颜色空间](https://blog.csdn.net/qq_31788759/article/details/106503745)

---
## GPU渲染管线及硬件架构笔记     
文章来源：[GPU 渲染管线和硬件架构浅谈](https://mp.weixin.qq.com/s/-ueKhxbsJOnUtV1SC5eyBQ)  

### GPU渲染管线     
---
**桌面端GPU渲染管线及架构**   
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/IMR%20pipeline.jpeg "IMR渲染架构")    
桌面端常见GPU架构，每个drawcall中，GPU按照顺序处理图元  
- 应用阶段：粗粒度剔除（遮挡剔除），渲染状态设置，顶点着色数据准备 （CPU内完成）   
- 几何阶段（顶点处理阶段）：顶点着色器，曲面细分，几何着色器，顶点裁剪，屏幕映射，该过程会做背面剔除裁剪    
- 光栅化阶段：三角形设置，三角形遍历，片元着色器   
- 逐片元操作：裁剪测试，深度测试，模板测试，混合操作  

- **优势**：   
渲染管线连续，有利于提高GPU最大吞吐量，最大化利用GPU性能，从vertex到raster处理都可以在GPU内部on-chip buffer上进行，所以只需少量带宽，存取处理过程中图元数据。     
- **劣势**：  
IMR是全屏绘制，当前绘制图元可能是屏幕的任意位置，因此需要全屏buffer，该buffer内存很大，只能放在系统内存中，这样在与系统内存buffer的数据进行交互时，需要消耗大量带宽，对于移动端来说过于昂贵。   

---  
**移动端渲染管线及架构**            
对于移动端，控制功耗很重要，功耗高带来发热降频，会是画面出现严重卡顿和掉帧。带宽的大量消耗会造成明显耗电和发热。因此移动端对于功耗控制尤为谨慎，普遍使用 TBR/TBDR 

- TBR:Tile-based Rendering      
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/TBR%20pipeline.jpeg)     
TBR不绘制整个屏幕，而是将屏幕分成不同的Tile，GPU每次只绘制一个Tile，绘制完后再将绘制结果写入系统内存的frameBuffer，处理过程：   
1. 处理所有的顶点，生成tile list中间数据，该数据保存顶点归属于屏幕的哪一个Tile   
2. 针对每一个Tile执行光栅化，片元着色器过程，每个Tile处理完毕之后，写入系统内存中     
- **优势**   
TBR减少了对系统内存的访问，减少了带宽开销，Tile足够小，可以在GPU on-chip-memory 上，访问速度快，后续的片元着色器阶段同样可以在 on-chip-memory上处理，depth buffer 和 stencil buffer 都只在Tile中处理才会用到，不用写入系统内存，进一步节省带宽      
- **劣势**   
TBR需要先对所有顶点进行处理，生成tile list ，这一步会产生明显的延迟，顶点越多消耗就越大，对于曲面细分在TBR下会显得昂贵，顶点量过大会带来性能严重下降        

- TBDR：Tile-based Deferred Rendering  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/TBDR%20pipeline.jpeg)    
TBDR 和 TBR 架构很类似，区别在于，TBDR多了一个隐面剔除，即 HSR&Depth Test这个步骤，通过HSR，最终只有对屏幕产生贡献的像素会执行片元着色器，被遮挡的片元会被丢弃，避免执行无效的片元着色器    
---   
### GPU 硬件架构     
---
**GPU 与 CPU 差异**     
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU_CPU差异.jpeg)      
- CPU 核心少（计算单元少，内部结构复杂），每个核心有控制单元和独立缓存，设计上是大缓存，要求低延时  
- GPU 计算单元多，多个计算单元共享控制单元和缓存，内存设计上是追求高带宽，可以接受高延时   
- CPU经常用来执行串行和计算复杂度高的任务，GPU则用来处理高并行，相互独立，复杂度低的任务，通过多核心来抵消任务处理过程中的任务排队时间     
---   
**CPU的缓存体系和指令执行过程**   
内存硬件类型     
- SRAM (Static Random Access Memory,静态随机存取内存) 静止存取数据作用，无需刷新电路即可保存数据，断电后数据消失，速度较DRAM快，一般用作片内缓存(on-chip-cache)，如L1,L2 Cache      
- DRAM（Dynamic Random Access Memory,动态随机存取内存）需要不断刷新电路，否则数据会消失。常用于做内存，容量比SRAM大，一般用作系统内存，桌面端目前都是DDR SDRAM(Double Data Rate Synchronous Dynamic Random-Access Memory)   
- GDDR (Graphic DDR) 用作显存，时钟频率更高，耗电量更少    
- LPDDR SDRAM (Low Power Double Date Rate),是移动设备常用低功耗SDRAM，以低功耗小体积著称，Frame Buffer存放于此     
- UFS 通用闪存存储，移动磁盘设备   

CPU缓存体系   
CPU有L1/L2/L3 三级缓存，L1与L2缓存在核心内部，L3为所有核心共享，缓存是SRAM，比系统内存DRAM速度快很多  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/CPU缓存.jpeg)    
- L1/L2 缓存是片上缓存，读取速度快，容量小。L1通常在32Kb - 256Kb,而L3缓存可以达到8Mb - 32Mb级别   
- L1级别缓存分为指令缓存(I-Cache)和数据缓存(D-Cache)，CPU针对指令和数据有不同缓存策略 
- L1缓存不能设计太大，增大L1缓存，会增加访问的时钟周期，对于低延时来讲，降低了L1 Cache性能  
- CPU的L1/L2缓存需要处理缓存一致性问题，不同核心之间L1缓存之间数据一致，一个核心中的L1缓存数据发生变化，其他核心中相应数据标记为无效，而GPU的缓存无需处理该问题（GPU缓存主要用来提高线程服务，而不是存储数据）   
- CPU的数据查找按照 L1->L2->L3->DRAM的顺序进行，当数据不在缓存中时，需要从系统内存中加载，会造成较大延时  
- 缓存对于CPU性能非常重要，很多时候缓存会占据芯片一半以上面积和晶体管。苹果A14/A5/M1 芯片性能的强劲与超大缓存密不可分     
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/CPU结构.jpeg)       
---
CPU指令执行过程  
- 指令流水线五个阶段   
    - 取指令(Instruction Fetch)从指令缓存(I-Cache)中取指令放入指令寄存器中    
    - 解码指令(Instruction Decode)，这里还通过寄存器文件(Register File)取到指令的源操作数    
    - 执行指令(Execute)  
    - 访问数据(Memory Access) 如果需要存储器取数据，通过数据缓存(D-Cache)取数据  
    - 结果写入(Register Write Back)将指令执行结果写入寄存器    

- 访问主存，CPU和GPU通常会出现较大延迟，面对延迟，CPU和GPU处理策略不同。CPU通过大容量高速缓存，分支预测，乱序执行遮掩延迟，CPU缓存容量大于GPU，GPU没有L3级缓存，且CPU缓存是以低延迟为目标设计。GPU则是通过大量的计算单元切换线程Wrap来规避延迟。   
- 分支，CPU通过分支预测，提高指令流水线的执行性能，GPU无分支预测单元，不擅长执行分支（在Shader中执行分支操作耗性能）。   
- 超标量设计，CPU可以同时发射多条指令，并使指令并行计算，充分利用计算单元。 
- 乱序执行，若按照原有指令顺序执行，可能指令之间有依赖无法并行执行，或者频繁出现高延迟指令。CPU会保证执行结果正确基础上，调整指令执行顺序，使指令更加高效执行，减少执行等待，提高管线性能。    
- 线程切换，CPU线程切换会有上下文切换的性能开销（线程切换需要保存寄存器和程序计数器，切换回来时需要恢复寄存器和程序计数器）CPU尽量避免频繁线程切换。GPU寄存器数量多，线程切换时不需要保存上下文，所以可以通过零成本切换线程来遮掩延迟。   
---
桌面端 GPU 硬件架构     
![NVIDIA Fermi架构](https://github.com/zhangbo-0213/PictureRepository/blob/main/NvidiaFermi.jpeg)   
不同的GPU，架构差异较大，一般包含以下核心组件：    
- SM，SMX，SMM(Streaming Multiprocessor)。GPU核心，执行 Shader 指令的地方，一个GPU由多个SM组成。Mali种类似单元叫 Shader Core，PowerVR 中为 Unified Shading Cluster(USC)。  
- Core 真正执行指令的地方，NVIDIA 中为CUDA Core  
- Raster Engine 光栅化引擎    
- ROP(Raster Operation) depth testing blending 等操作在这里完成   
- Register File ,L1 Cache L2 Cache 寄存器和各级缓存    
---
Shader Core 主要构成单元    
- 32个 运算核心 (CUDA Core,也叫 流处理器 Stream processor)    
- 16个 LD/ST (Load/Store) 模块来加载和存储数据 
- 4个 SFU(Special function units) 执行特殊数学计算(sin,cos,log)等    
- 128K 寄存器 (Register File) 3万个 32-Bit 寄存器，大寄存器设计      
- 64K L1缓存 (on-chip Memory)  
- 纹理读取单元 (Texture Unit)  
- 指令缓存 (Instruction Cache)  
- Wrap Schedules:这个模块负责 Wrap 调度,一个wrap由32个线程组成，wrap调度器的指令Dispatch Unit 送到 Core 执行 
---
GPU 内存结构     
- UMA(Unified Memory Architeture)        
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU内存结构.jpeg)    
  - 左图为桌面端独显的分离式架构，CPU和GPU使用独立的物理内存，右侧为移动端的统一内存架构，CPU与GPU共用一个物理内存     
  - UMA 并不是指CPU，GPU内存合并在一起，实际上他们使用的内存区域不一样，物理内存中有一块区域由GPU自己管理。CPU和GPU的数据通信依然有拷贝过程。   
  - 移动芯片都是Soc (System on Chip),CPU和GPU等元件在一个芯片上，芯片面积不能做的很大，不能像桌面端为显卡配置GDDR显存，并且通过独立的北桥 PCI-e 通信。移动端CPU和GPU使用同一个物理内存，更加灵活，操作系统可以决定分配给GPU的显存大小，但是会造成CPU和GPU抢占带宽，会进一步限制GPU能使用的带宽     
  - GPU使用独立的显存空间可以对Buffer 或者 Texture 做进一步优化   

GPU 缓存分类      

- GPU 缓存结构  
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU缓存结构.jpeg)     
  - L1缓存是片上缓存(on-chip)，每个 shader 核心都有自己独立的 L1 缓存，访问速度快。移动GPU会有Tile Memory，也就是片上内存(on-chip memory)   
  - L2缓存是所有shader核心共享，属于片外缓存，距离shader核心比L1缓存远，访问速度较L1缓存慢   
  - DRAM 是主存(系统内存，System Memory)，访问速度最慢，Frame Buffer放在主存上。   

- 内存访问速度    
内存访问速度从寄存器到主存依次变慢    
![](https://github.com/zhangbo-0213/PictureRepository/blob/main/内存访问速度.png)     
  - 寄存器访问速度是最快的，GPU的寄存器数量很多    
  - Shared Memory和L1 Cache是同一个硬件单元，Shad red Memory 是可以由开发者控制的片上内存，L1缓存是GPU控制的，开发者无法访问，部分移动端芯片没有Shared Memory   
  - Local Memory和Texture/Const Memory 都是主存上的内存区域，访问速度比较慢      

NVIDIA 的内存分类      
NVIDIA 的内存分类主要是针对 CUDA 开发的，与游戏开发或者移动GPU的分类叫法存在部分差异     
- 全局内存(Global memory),即主存  
- 本地内存(LOcal memory),是Global中的一部分，每个线程私有，主要用于处理寄存器溢出，或者超大uniform数组，访问速度很慢   
- 共享内存(Shared memory),Shared memory 是片上内存，访问速度较快，是一个shader核心内所有线程共享   
- 寄存器内存(Register memory)，访问速度最快   
- 常量内存(Const memory),Const memory和Local memory类似，是Global memory 中的一块区域，访问速度较慢    
- 纹理内存(Texture memory),与 Const memory一样，也在主存上         

Cache line  
 - GPU 和缓存之间的内存交换不是以字节为单位，而是以 Cache line为单位，Cache line是固定的大小，比如CPU的Cache line 是64字节，GPU是128字节     
 - Cache Line的数据内容为：标记位+地址偏移+实际数据，标记位是为了确定缓存是否命中，是否写入主存    

Memory Bank 和 Bank Conflict   
 - 为提高内存的访问性能，获得更高的带宽，Shared Memory/L1 Cache 被设计成更小块的bank区域（bank可以理解为Memory 的对外窗口，多个窗口可访问比只有一个窗口要高效），bank 数量一般与wrap大小或者CUDA core的数量对应，如32个core就把SMEM划分为32个bank,每个bank包含多个cache line      
 - 如果一个wrap中的不同线程访问的是不同的bank,那么可以并行执行，最大化利用带宽，提高性能    
 - 如果访问的是同一个bank中的同一个cache line，通过广播机制同步到其他线程，这样一次访问，其他线程都能获取到数据，不会存在性能问题   
 - 如果访问的是同一个bank上的不同cache line，会形成阻塞，必须阻塞等待，串行访问，这种情况会阻碍GPU的并行性，性能会下降，被称为 bank conflict   
 - 如果不同线程对同一个cache line 产生写入操作，也必须阻塞等待，串行写入        
 ---  
 GPU 运算系统    
 - SIMD(single instruction multiple data)和 SIMT(single instruction multiple thread)    
   - 游戏引擎内，常使用SSE加速计算(如视锥体裁剪计算) ，利用的是SIMD，单指令计算多个数据     
   - GPU的设计为了满足大规模并行计算，因此GPU是典型的SIMD/SIMT 执行模式，GPU内部将若干相同的输入打包成一组并行执行   
   - Vector processor和Scalar processor 概念。早期GPU是Vector processor(对应SIMD)，因为早期处理的是颜色数据，处理rgba四个分量，编译器尽可能将数据打包成vec4来进行计算，随着图形渲染及GPU发展，处理的数据越来多样复杂，数据不一定有必要打包成vec4,从而可能导致性能浪费。因此现代GPU改进为Scalar processor(对应SIMT模式)  
   ![](https://github.com/zhangbo-0213/PictureRepository/blob/main/Vector与Scalar处理单元.jpeg)  
   上图左侧为vector处理器，右侧为scalar处理器。对于vector处理器而言，是在一个cycle内计算(x,y,z)值，如果没有被被填满，将会造成空间浪费，而Scalar处理器是在3个时钟周期内分别计算x,y,z的值，可以同时支持4线程计算，不会造成处理器性能浪费。
   - 现代GPU为SIMT执行架构。传统SIMD是一个线程调用向量处理单元(Vector ALU)操作向量寄存器完成运算，而SIMT是由一组标量处理单元(Scalar ALU)构成，每个处理单元对应一个像素线程，所有ALU共享控制单元，如取指令/译码模块，他们接收同一个指令共同完成运算，每个线程有自己的寄存器，独立的内存访问地址以及执行分支。    
   - 传统的SIMD是数据级并行，DLP(Data Level Parallelism)，SIMT是线程级并行 TLP(Thread Level Parallelism),更进一步的超标量(Super Scalar)是指令级并行,ILP(Instruction Level Parallelism)   
   - PowerVR、Adreno的GPU，以及Mali最新的Valhall架构的GPU都支持Super Scalar，可以同时发射多个指令，由空闲ALU执行，即同一个pipeline内的多个ALU元件可以并行执行指令序列中的指令   
   - 无论哪种架构，GPU的计算单元都是并行处理大量的数据，有时会直接把GPU的计算单元称作SIMD引擎     
- Wrap线程束   
   - Wrap是典型的单指令多线程(SIMT)的实现。32个线程为一组线程束，32个线程同时执行一样的指令，只是线程数据不一样，这就是锁步(lock step)执行,这样一个Wrap只需要一套控制单元对指令进行解码和执行，芯片可以做的更小更快。
   - Wrap Scheduler 线程束调度器会将数据存入寄存器，然后将着色器代码存入指令缓存，要求指令分派单元(DispatchUnit)从指令缓存读取指令分派给计算核心(ALU)执行   
   - Wrap中所有线程执行的都是相同的指令，如果遇到分支，就可能出现线程不激活执行的情况(如当前指令是true的分支，但是当前线程数据条件是false),此时线程会被遮掩(mask out)，其执行的结果会被丢弃。shader 中出现分支会显著增加时间消耗，在一个Wrap中的分支除非32个线程都走到if或者else里面，否则相当于所有的分支都会走一遍，然后根据条件丢弃执行结果   
   - 一个Wrap中的像素线程可以是不同的图元，其shader指令是一致的   
   - Wrap中的线程数量和SM中的CUDA core的数量并不一定是一致的，可以是Wrap为32，但是CUDA core 只有16个，这种情况下，每个core，两个cycle完成一个warp计算，power VR为类似设计    
- Stall 和 Latency Hiding（延迟隐藏）   
  - 指令从开始到结束消耗的 clock cycle 称为指令的  latency。延迟通常是由对主存的访问产生的，比如纹理采样、读取顶点数据，读取 Varying 等。对于纹理采样，如果cache missing 的情况可能需要消耗几百个时钟周期。   
  -  CPU 通过分支预测、乱序执行、大容量缓存技术来隐藏延迟，GPU通过wrap切换来隐藏延迟。   
  - 对 CPU而言，上下文切换具有明显开销，所以CPU尽可能避免频繁线程切换。而GPU在wrap之间切换几乎无性能开销，所以当一个wrap stall了，GPU切换到下一个Wrap，之前的Wrap获得所需数据后，切换回来继续执行。这种机制是基于GPU大寄存器设计，GPU中的寄存器数量远超CPU。 
  ![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU%20stall.jpeg)   
  -  每个Shader Core会被同时分配多个Wrap来执行，Wrap一旦进入Shader Core 中就不会离开，直到执行完毕。每个线程会在一开始分配好所需寄存器和Local Memory。当一个wrap产生Stall，GPU的core会直接切换到另外的wrap来执行，由于不需要保存和恢复寄存器状态，这个切换几乎没有成本，可以在一个cycle 内完成。  
  - Shader Core中的wrap调度器每个cycle会挑选Active wrap 执行，被选中的为 Selected wrap,没被选中，但是已经做好准备执行的称为 Eligible wrap，没准备好执行的称为 Stall wrap。wrap 准备好执行需要满足的两个条件：CUDA core 有空闲，所有当前指令的参数准备就绪。     
  - 如果Shader中使用的变量越多，占用的寄存器数量就越多，留给 wrap 切换的寄存器就会变少，从而 分配给 Shader Core 的Wrap 数量就减少了，Active wrap 就会降低，这样会降低GPU延迟隐藏的能力，降低利用率。 

- Wrap Divergence     
  - 由于Wrap是锁步执行的，Wrap中32个线程执行的是同样的指令，当shader中有 if-else 时，如果有的线程需要走if分支，有的线程需要走else分支，就会出现 divergence，GPU的处理方式是两个分支都走一遍，通过 Mask 遮蔽掉不需要的结果。    
  - 如果Wrap所有线程走的是分支的一侧，就没有太大的影响。如果两条分支都走，对性能影响较大     
  ![](https://github.com/zhangbo-0213/PictureRepository/blob/main/GPU分支.jpeg)    

---    
其他概念       
 - Pixel Quad   
   - 光栅化阶段，栅格离散化的粒度最终虽然是像素级，但是离散化模块的输出单位并不是单个像素，而是 Pixel Quad (2x2 像素),因为单个的像素无法计算 ddx,ddy ，进行Early Z 判定的最小单位也是 Pixel Quad。        
   ![](https://github.com/zhangbo-0213/PictureRepository/blob/main/Pixel%20Quad.jpeg)       
   - 上图所示中，像素点网格被划分了 2x2 的组，这个划分组为Quad。一个三角形，即使只覆盖了一个Quad中的一个像素，整个Quad中的四个像素都需要执行像素着色器。Quad中未被覆盖的像素称为"辅助像素"。比较小的三角形在渲染时，辅助像素的比例会更高，从而造成性能浪费。      
   - 辅助像素依然在管线内参与整个像素着色器运算，只是计算结果会被丢弃，由于GPU与内存之间的cache line 的存在，一次交换数据的量固定，这些被丢弃的像素很多时候也不会节省带宽，会原样读入，原样输出，带宽的消耗还是在，因此尽量避免大量小图元的绘制，能更加有效利用Wrap。  
- EarlyZS     
  - 深度测试 DepthTest 和 模版测试 StencilTest 是一个硬件单元 (ROP中的硬件单元)。Early Depth Test 阶段同样可以做 Early Stencil Test，因此该过程也被叫做 Early ZS。Early-Z 可以将无效像素提前剔除，避免进入耗时严重的像素着色器，Early-Z的提最小剔除单位不是1像素，而是1 pixel quad(4像素)。  
  - 传统管线渲染中，depth Test会在像素着色器之后进行。depth Test 时，当发现自身被遮挡，会被丢弃。之前的计算就会造成性能浪费。现代GPU中使用了 Early- Z 的技术，在像素着色器之前，先进行一次深度测试，如果深度测试失败，就不用再进行像素着色器，因此这样在性能上就会有较大提升。    
  - Alpha Test 会影响 Early-Z 的执行。一方面自身不能执行 Early-Z Write 操作,因为只有当像素着色器执行完毕之后才会知道会不会被丢弃，如果通过Early- Z提前写入深度会有错误的执行结果，另一方面只有自己执行完像素着色器，写入深度之后，相同位置的后续片元才能继续执行，否则就会阻塞等待返回结果。   
  - 对于在像素着色器中手动修改深度插值结果，使用clip，discard等操作，都会影响Early-Z的执行。    
- Hierarchical-z 和 Tile-based Rasteration  
  这是由硬件提供的优化 
  - Hierarchical Z-Culling,也叫做Z-Cull。是NVIDIA硬件支持的粗粒度裁剪方案。类似于 Adreno 的 LRZ 技术，通过低分辨率的Z-Buffer来做剔除，只能精确到8X8像素块，而不是LRZ的精确到 Quad(2x2)，移动平台有其他技术做裁剪剔除，同时，它和Early-Z不同。    
  - Tile-based Rasteration技术，光栅化也能Tile- Based，这同样是硬件厂商的优化技术。光栅化阶段通通常不会成为性能瓶颈。原神曾对树叶使用Stencil，期望通过抠图实现半透明效果来提高性能，但是因为影响到Tile- based Rastoration 的优化，反而导致性能下降。在PC平台原本一些优化技术，比如通过 discard 或者其他手段剔除掉像素，避免进入像素着色器或者ROP（减少访问主存）阶段，以此来提高性能，这些优化手段在移动平台通常是负优化。   
  - GPU对于大量小三角形绘制非常不擅长。GPU对顶点着色器和光栅化的优化手段有限，同时光栅化的输出最小单元是PixelQuad,大量像素级的小三角形就会必然导致wrap中的有效像素大大减少。UE5的Nanite会使用ComputeShader自己实现软光栅，来代替硬件光栅化处理这些像素级的小三角形，来获得几倍的性能提升。    
- Register Spilling 和 Active Wrap   
  - GPU的寄存器虽然很多，数量还是有限。GPU的核心在执行一个Wrap是，会在开始把寄存器分配给每个线程，如果shader占用过多寄存器，留给GPU核心用来执行Wrap线程的寄存器数量就会减少，从而Active Wrap降低。这样会降低GPU隐藏延迟的能力，从而影响GPU的性能。如，原本在一个Wrap加载纹理产生Stall时，会切换到下个Wrap，如果Active Wrap太少，就有可能所有的Wrap都在等纹理加载，此时GPU核心产生真正的Stall，只能等待结果返回。  
  - 寄存器文件使用量在shader编译完就可以确定。每个变量，临时变量，部分符合条件的 uniform 变量，都会占用寄存器文件。如果shader占用的寄存器过多，比如超过64或者128，会产生更加严重的性能问题，就是 Register Spilling (寄存器溢出)。GPU会将寄存器文件存储到 Local Memory 上，Local Memory 是主存的一块区域，访问速度很慢，Register Spilling 会大大降低shader 的执行性能。（shader 变量过多会影响自身的执行性能）。   
Mipmap   
  - 传递给 GPU 一个带 Mipmap的纹理，GPU在运行时通过(ddx,ddy)偏导选取合适的Mipmap Level 的纹理。
  - Mipmap 有利于节省带宽，并不是指 传递的数据量变小（实际上是变多了1/3），而是最终渲染的时候，相邻的像素更有可能在一个cache line 里面，提高了Texture cache 的命中率，减少与主存的交互，因此减少带宽。  
  - 当需要访问主存时，需要消耗几百个时钟周期，会产生严重的Stall。而提升texture cache 的命中率就可以减少这种情况的发生。在通过性能分析工具优化时，texture L1/L2 cache missing 是一个非常重要的指标，通常要控制在较低值才是合理的。  
  -  Mipmap本身会多占用1/3的内存（低级别的mipmap图），开发者可以决定upload给GPU的最高mipmap level 。通过引擎动态控制纹理的最高 mipmap level ，反而可以有效控制纹理占用的内存用量，在许多3D游戏场景中，贴图由模糊变清晰，可能就是在动态改变mipmap纹理的level。   
  - 移动平台的GPU内存和CPU内存是共用一块物理内存，但其内存空间是分离的，因此纹理提交给GPU是需要upload，当纹理upload到GPU后，CPU端的纹理内存会被释放掉，这是，若将显存的纹理内存释放掉，也就相当于释放了纹理内存。  
  - Unity中，一部分纹理是需要CPU端读写数据的，或者编辑器下某个纹理导入选项中勾选了Read/Write Enabled。对这些纹理而言，CPU端的内存不会被释放掉，此时该纹理就占用了两份内存，CPU一份，GPU一份。      

  











